{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 299: Exampville Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import larch\n",
    "from itertools import tee\n",
    "def pairwise(iterable):\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    yield from zip(a, b)\n",
    "\n",
    "larch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Exampville, the best simulated town in this here part of the internet!\n",
    "\n",
    "Exampville is provided with Larch, and uses the\n",
    "kind of data that a transportation planner might have available when building\n",
    "a travel model.  However, this data is entirely fictional.\n",
    "\n",
    "This page walks through the creation of this synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import larch.exampville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travel Analysis Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a shape file delineating some travel analysis zones. \n",
    "(Five bonus points if you can identify the real American city \n",
    "from which these tract shapes were derived.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_shape = gpd.read_file(\"zip://\"+larch.exampville.files.shapefile)\n",
    "ax = taz_shape.plot(edgecolor='w', figsize=(10,10), cmap='tab20')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "# Put Zone labels in the meatiest part of the zone shape...\n",
    "for idx, row in taz_shape.iterrows():\n",
    "    shrink = row['geometry']\n",
    "    while True:\n",
    "        try: \n",
    "            shrink1 = shrink.buffer(-5)\n",
    "            if shrink1.area <= 0:\n",
    "                break\n",
    "            else:\n",
    "                shrink = shrink1\n",
    "        except:\n",
    "            break\n",
    "    ax.annotate(\n",
    "        text=row['TAZ'], \n",
    "        xy=tuple(shrink.representative_point().coords)[0], \n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_income_tazs = [2,33,34,5]\n",
    "lower_income_tazs = [40,29,30,17,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nZones = len(taz_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_cbd = [36]\n",
    "zones_urb = [4,28,25,3,35,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hh = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Group 1: In-town density\n",
    "n_hh_1 = 3500\n",
    "\n",
    "mean = [650, 400, 11, 0.35, 0.52]\n",
    "i_y = 80\n",
    "i_x = 100\n",
    "v_y = -22\n",
    "v_x = 10\n",
    "s_x = 10\n",
    "s_y = -22\n",
    "s_v = 0.21\n",
    "s_i = 0.1\n",
    "cov = [\n",
    "    [44000, 25000, i_x,  v_x,  s_x], \n",
    "    [25000, 44000, i_y,  v_y,  s_y], \n",
    "    [  i_x,   i_y, 1.0,  0.4,  s_i],\n",
    "    [  v_x,   v_y, 0.4, 0.35,  s_v],\n",
    "    [  s_x,   s_y, s_i,  s_v, 0.35],\n",
    "] \n",
    "\n",
    "x, y, income, veh, hhsz = np.random.multivariate_normal(mean, cov, n_hh_1).T\n",
    "\n",
    "\n",
    "\n",
    "# Assemble into a DataFrame\n",
    "hh_locations = pd.DataFrame.from_dict({\n",
    "    'X':np.round(x,2),\n",
    "    'Y':np.round(y,2),\n",
    "    'INCOME':np.exp(income).astype(int),\n",
    "    'N_VEHICLES':np.exp(veh).astype(int),\n",
    "    'HHSIZE':np.exp(hhsz).astype(int)+1,\n",
    "})\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "hh_locations = gpd.GeoDataFrame(\n",
    "    hh_locations, \n",
    "    geometry=gpd.points_from_xy(hh_locations.X, hh_locations.Y),\n",
    "    crs={},\n",
    ")\n",
    "\n",
    "# Attach HOMETAZ, and drop points outside the region.\n",
    "hh_locations = gpd.sjoin(hh_locations, taz_shape[['TAZ','geometry']], how='inner', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations.HHSIZE.statistics(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.pivot_table(\n",
    "    hh_locations,\n",
    "    index='HHSIZE', \n",
    "    columns='N_VEHICLES',\n",
    "    aggfunc={'X':'size'},\n",
    ").fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = taz_shape.plot(edgecolor='k', figsize=(10,10), color='w')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "hh_locations.plot(ax=ax, column='INCOME', alpha=1, cmap='Reds', vmax=150000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = taz_shape.plot(edgecolor='k', figsize=(10,10), color='w')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "hh_locations.plot(ax=ax, column='N_VEHICLES', alpha=1, cmap='Reds', vmax=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations.INCOME.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Group 2: Regional base population\n",
    "\n",
    "n_hh_2 = n_hh - len(hh_locations) # Enough to get back to the desired total\n",
    "\n",
    "mean = [650, 400, 20000, 0.45, 0.52]\n",
    "i_y = 0\n",
    "i_x = 0\n",
    "v_y = 0\n",
    "v_x = 0\n",
    "s_x = 0\n",
    "s_y = 0\n",
    "v_i = 846\n",
    "s_v = 0.28\n",
    "s_i = 600\n",
    "cov = [\n",
    "    [44000,     0,    i_x,  v_x,  s_x], \n",
    "    [    0, 44000,    i_y,  v_y,  s_y], \n",
    "    [  i_x,   i_y,9000000,  v_i,  s_i],\n",
    "    [  v_x,   v_y,    v_i, 0.25,  s_v],\n",
    "    [  s_x,   s_y,    s_i,  s_v, 0.35],\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2, y2, i2, v2, s2 = np.random.multivariate_normal(mean, cov, n_hh_2).T\n",
    "\n",
    "\n",
    "x2 = np.random.random(n_hh_2)*1000\n",
    "y2 = np.random.random(n_hh_2)*800\n",
    "# i2 = np.exp(np.random.random(n_hh_2)*3+8).astype(int)\n",
    "# v2 = np.exp(np.random.random(n_hh_2)*1.3)\n",
    "# s2 = v2 + np.exp(np.random.random(n_hh_2)*0.3)\n",
    "\n",
    "# Assemble into a DataFrame\n",
    "hh_locations2 = pd.DataFrame.from_dict({\n",
    "    'X':np.round(x2,2),\n",
    "    'Y':np.round(y2,2),\n",
    "    'INCOME':(i2),\n",
    "    'N_VEHICLES':np.exp(v2).astype(int),\n",
    "    'HHSIZE':np.exp(s2).astype(int)+1,\n",
    "})\n",
    "\n",
    "# Convert to a GeoDataFrame\n",
    "hh_locations2 = gpd.GeoDataFrame(\n",
    "    hh_locations2, \n",
    "    geometry=gpd.points_from_xy(hh_locations2.X, hh_locations2.Y),\n",
    "    crs={},\n",
    ")\n",
    "\n",
    "# Attach HOMETAZ\n",
    "hh_locations2 = gpd.sjoin(hh_locations2, taz_shape[['TAZ','geometry']], how='inner', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations2.INCOME.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations2.N_VEHICLES.statistics(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_locations2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge Groups\n",
    "\n",
    "HH = pd.concat([hh_locations, hh_locations2], sort=False)\n",
    "\n",
    "# Clean up\n",
    "HH = HH.reset_index(drop=True)\n",
    "HH.rename({'TAZ':'HOMETAZ'}, axis=1, inplace=True)\n",
    "HH.drop('index_right', axis=1, inplace=True)\n",
    "\n",
    "# Add HH size and ID\n",
    "# HH['HHSIZE'] = (np.floor(\n",
    "#     np.random.binomial(5, 0.3, [n_hh, ]) \n",
    "#     + 1 \n",
    "#     + np.random.random([n_hh, ])\n",
    "# )).astype(int)\n",
    "\n",
    "#np.floor(np.random.exponential(0.8, [n_hh, ]) + 1 + np.random.random([n_hh, ])).astype(int)\n",
    "HH['HHID'] = HH.index + 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HH.index = HH['HHID']\n",
    "HH['HHSIZE'].statistics(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HH['INCOME'].statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HH.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = taz_shape.plot(edgecolor='k', figsize=(10,10), color='w')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_frame_on(False)\n",
    "HH.plot(ax=ax, color='red', alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HHsize = HH['HHSIZE']\n",
    "n_PER = np.sum(HHsize)\n",
    "\n",
    "PER = {}\n",
    "PERidx = PER['idx'] = np.arange(n_PER, dtype=np.int64)\n",
    "PERid = PER['PERSONID'] = np.asarray([60000 + i for i in PER['idx']])\n",
    "PERhhid = PER['HHID'] = np.zeros(n_PER, dtype=np.int64)\n",
    "PERhhidx = PER['HHIDX'] = np.zeros(n_PER, dtype=np.int64)\n",
    "PERage = PER['AGE'] = np.zeros(n_PER, dtype=np.int64)\n",
    "n2 = 0\n",
    "for n1 in range(n_hh):\n",
    "    PER['HHID'][n2:(n2 + HHsize[n1])] = HH['HHID'][n1]\n",
    "    PER['HHIDX'][n2:(n2 + HHsize[n1])] = HH.index[n1]\n",
    "    \n",
    "    PER['AGE'][n2] = int(np.random.random() * 67 + 18)\n",
    "    if HHsize[n1]>1:\n",
    "        nphh = HHsize[n1]-1\n",
    "        PER['AGE'][n2+1:n2+1+nphh] = (np.random.random(nphh) * 80 + 5).astype(np.int64)\n",
    "    n2 += HHsize[n1]\n",
    "    \n",
    "    \n",
    "    \n",
    "PERworks = PER['WORKS'] = ((np.random.random(n_PER) > 0.15) & (PERage > 16) & (PERage < 70)).astype(np.int64)\n",
    "\n",
    "PER = pd.DataFrame.from_dict(PER)\n",
    "PER.drop('idx', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER['AGE'].statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HH.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_veh = HH[['N_VEHICLES', 'INCOME']]\n",
    "n_veh.index = HH.HHID\n",
    "PER = pd.merge(PER, n_veh, left_on='HHID', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcounts = PER.N_VEHICLES.value_counts().sort_index()\n",
    "vcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER.N_VEHICLES.statistics(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERnworktours = np.zeros_like(PERage)\n",
    "PERnothertours = np.zeros_like(PERage)\n",
    "i = 0\n",
    "for nv,nt in vcounts.items():\n",
    "    if nv==0:\n",
    "        work_rates, other_rates = [0.25, 0.70, 0.04, 0.01], [0.74, 0.2, 0.05, 0.01]\n",
    "    elif nv==1:\n",
    "        work_rates, other_rates = [0.13, 0.74, 0.09, 0.04], [0.2, 0.5, 0.2, 0.1]\n",
    "    elif nv==2:\n",
    "        work_rates, other_rates = [0.10, 0.70, 0.17, 0.03], [0.1, 0.5, 0.3, 0.1]\n",
    "    elif nv==3:\n",
    "        work_rates, other_rates = [0.07, 0.61, 0.26, 0.06], [0.1, 0.4, 0.3, 0.2]\n",
    "    else:\n",
    "        work_rates, other_rates = [0.05, 0.53, 0.33, 0.09], [0.1, 0.3, 0.4, 0.2]\n",
    "    \n",
    "    work_rates = np.array(work_rates)\n",
    "    other_rates = np.array(other_rates)\n",
    "    \n",
    "    for income_under, income_over in pairwise([np.inf, 150_000, 120_000, 90_000, 60_000, 30_000, -np.inf]):\n",
    "        PERv = (PER.N_VEHICLES == nv) & (PER.INCOME>=income_over) & (PER.INCOME<income_under)\n",
    "        if income_over == 150_000:\n",
    "            work_rates_ = work_rates + np.array([-0.04, -0.10, 0.10, 0.04])\n",
    "            other_rates_ = other_rates + np.array([-0.07, -0.13, 0.13, 0.07])\n",
    "        elif income_over == 120_000:\n",
    "            work_rates_ = work_rates + np.array([-0.03, -0.08, 0.08, 0.03])\n",
    "            other_rates_ = other_rates + np.array([-0.05, -0.1, 0.13, 0.02])\n",
    "        elif income_over == 90_000:\n",
    "            work_rates_ = work_rates + np.array([-0.02, -0.06, 0.07, 0.01])\n",
    "            other_rates_ = other_rates + np.array([-0.03, -0.07, 0.08, 0.02])\n",
    "        elif income_over == 60_000:\n",
    "            work_rates_ = work_rates + np.array([-0.01, -0.04, 0.05, 0.0])\n",
    "            other_rates_ = other_rates + np.array([-0.02, -0.03, 0.04, 0.01])\n",
    "        elif income_over == 30_000:\n",
    "            work_rates_ = work_rates \n",
    "            other_rates_ = other_rates \n",
    "        else:\n",
    "            work_rates_ = work_rates + np.array([0.1, 0.05, -0.05, -0.1])\n",
    "            other_rates_ = other_rates + np.array([0.2, 0.05, -0.05, -0.2])\n",
    "        j = sum(PERv)    \n",
    "        \n",
    "        work_rates_[work_rates_<0.0001] = 0.0001\n",
    "        work_rates_ /= work_rates_.sum()\n",
    "\n",
    "        other_rates_[other_rates_<0.0001] = 0.0001\n",
    "        other_rates_ /= other_rates_.sum()\n",
    "\n",
    "        PERnworktours[PERv] = np.random.choice([0, 1, 2, 3], size=[j, ], replace=True, p=work_rates_).astype(np.int64) * PER['WORKS'][PERv]\n",
    "        PERnothertours[PERv] = np.random.choice([0, 1, 2, 3], size=[j, ], replace=True, p=other_rates_).astype(np.int64)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for income_under, income_over in pairwise([np.inf, 150_000, 120_000, 90_000, 60_000, 0]):\n",
    "    print(income_under, income_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add counts of tours\n",
    "\n",
    "# PER['N_WORK_TOURS'] = PERnworktours = np.random.choice([0, 1, 2, 3], size=[n_PER, ], replace=True, p=[0.1, 0.8, 0.07, 0.03]).astype(\n",
    "#     np.int64) * PER['WORKS']\n",
    "# PER['N_OTHER_TOURS'] = PERnothertours = np.random.choice([0, 1, 2, 3], size=[n_PER, ], replace=True, p=[0.2, 0.5, 0.2, 0.1]).astype(np.int64)\n",
    "\n",
    "PER['N_WORK_TOURS'] = PERnworktours\n",
    "PER['N_OTHER_TOURS'] = PERnothertours\n",
    "PER['N_TOURS'] = PERntours = PERnworktours + PERnothertours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER.INCOME.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PER.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_employment = PER['WORKS'].sum()\n",
    "total_employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Group 1 Retail Locus\n",
    "\n",
    "n_retail_jobs_1 = int(total_employment * 0.1)\n",
    "\n",
    "mean = [370, 560,]\n",
    "cov = [\n",
    "    [3000, 0], \n",
    "    [0, 3000], \n",
    "] \n",
    "x, y = np.random.multivariate_normal(mean, cov, n_retail_jobs_1).T\n",
    "retail_locations_1 = pd.DataFrame.from_dict({\n",
    "    'X':x,\n",
    "    'Y':y,\n",
    "})\n",
    "\n",
    "retail_locations_1 = gpd.GeoDataFrame(\n",
    "    retail_locations_1, \n",
    "    geometry=gpd.points_from_xy(retail_locations_1.X, retail_locations_1.Y),\n",
    "    crs={},\n",
    ")\n",
    "retail_locations_1 = gpd.sjoin(retail_locations_1, taz_shape[['TAZ','geometry']], how='inner', op='within')\n",
    "\n",
    "ax =taz_shape.plot(edgecolor='k', color='w')\n",
    "retail_locations_1.plot(ax=ax, color='red', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Group 2 Retail Background\n",
    "\n",
    "n_retail_jobs_2 = int(total_employment * 0.1)\n",
    "\n",
    "mean = [600, 250,]\n",
    "cov = [\n",
    "    [22000, 0], \n",
    "    [0, 22000], \n",
    "] \n",
    "x, y = np.random.multivariate_normal(mean, cov, n_retail_jobs_2).T\n",
    "retail_locations_2 = pd.DataFrame.from_dict({\n",
    "    'X':x,\n",
    "    'Y':y,\n",
    "})\n",
    "\n",
    "retail_locations_2 = gpd.GeoDataFrame(\n",
    "    retail_locations_2, \n",
    "    geometry=gpd.points_from_xy(retail_locations_2.X, retail_locations_2.Y),\n",
    "    crs={},\n",
    ")\n",
    "retail_locations_2 = gpd.sjoin(retail_locations_2, taz_shape[['TAZ','geometry']], how='inner', op='within')\n",
    "\n",
    "ax =taz_shape.plot(edgecolor='k', color='w')\n",
    "retail_locations_2.plot(ax=ax, color='red', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Group 3 Downtown Locus\n",
    "\n",
    "n_other_jobs_1 = int(total_employment * 0.5)\n",
    "\n",
    "mean = [300, 250,]\n",
    "cov = [\n",
    "    [22000, 0], \n",
    "    [0, 22000], \n",
    "] \n",
    "x, y = np.random.multivariate_normal(mean, cov, n_other_jobs_1).T\n",
    "other_locations_1 = pd.DataFrame.from_dict({\n",
    "    'X':x,\n",
    "    'Y':y,\n",
    "})\n",
    "\n",
    "other_locations_1 = gpd.GeoDataFrame(\n",
    "    other_locations_1, \n",
    "    geometry=gpd.points_from_xy(other_locations_1.X, other_locations_1.Y),\n",
    "    crs={},\n",
    ")\n",
    "other_locations_1 = gpd.sjoin(other_locations_1, taz_shape[['TAZ','geometry']], how='inner', op='within')\n",
    "\n",
    "ax =taz_shape.plot(edgecolor='k', color='w')\n",
    "other_locations_1.plot(ax=ax, color='red', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Group 4 Background\n",
    "\n",
    "n_other_jobs_2 = total_employment - len(other_locations_1) - len(retail_locations_2) - len(retail_locations_1)\n",
    "\n",
    "x2 = np.random.random(n_other_jobs_2)*1000\n",
    "y2 = np.random.random(n_other_jobs_2)*800\n",
    "other_locations_2 = pd.DataFrame.from_dict({\n",
    "    'X':x2,\n",
    "    'Y':y2,\n",
    "})\n",
    "other_locations_2 = gpd.GeoDataFrame(\n",
    "    other_locations_2, \n",
    "    geometry=gpd.points_from_xy(other_locations_2.X, other_locations_2.Y),\n",
    "    crs={},\n",
    ")\n",
    "\n",
    "other_locations_2 = gpd.sjoin(other_locations_2, taz_shape[['TAZ','geometry']], how='inner', op='within')\n",
    "\n",
    "ax =taz_shape.plot(edgecolor='k', color='w')\n",
    "other_locations_2.plot(ax=ax, color='red', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge all employment\n",
    "\n",
    "retail_locations_1['JOBTYPE'] = 'retail'\n",
    "retail_locations_2['JOBTYPE'] = 'retail'\n",
    "other_locations_1['JOBTYPE'] = 'nonretail'\n",
    "other_locations_2['JOBTYPE'] = 'nonretail'\n",
    "\n",
    "\n",
    "job_location = pd.concat([retail_locations_1,retail_locations_2,other_locations_1,other_locations_2])\n",
    "\n",
    "job_location = job_location.reset_index(drop=True)\n",
    "job_location['JOBTYPE'] = job_location['JOBTYPE'].astype('category')\n",
    "\n",
    "ax =taz_shape.plot(edgecolor='k', color='w')\n",
    "job_location.plot(ax=ax, color='red', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment = job_location.groupby(['TAZ','JOBTYPE']).size().unstack().fillna(0).astype(int)\n",
    "taz_employment.rename({'retail':'RETAIL_EMP', 'nonretail':'NONRETAIL_EMP'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment['TOTAL_EMP'] = taz_employment['NONRETAIL_EMP'] + taz_employment['RETAIL_EMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_employment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "taz_employment.to_csv(larch.exampville._files(seed).employment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_route = [12,2,10,23,24,26,27,39,11]\n",
    "transit_line = [39,6,36,25,29,17,31,14,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_road = 3 # minutes per mile\n",
    "speed_highway = 1 # minutes per mile\n",
    "speed_train = 1.2 # minutes per mile\n",
    "speed_walk = 20 # minutes per mile\n",
    "speed_bike = 5 # minutes per mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph()\n",
    "\n",
    "for index, zone in taz_shape.iterrows():   \n",
    "    # get 'not disjoint' countries\n",
    "    neighbors = taz_shape[~taz_shape.geometry.disjoint(zone.geometry)].TAZ.tolist()\n",
    "    neighborc = taz_shape[~taz_shape.geometry.disjoint(zone.geometry)].geometry.centroid.tolist()\n",
    "\n",
    "    # add names of neighbors as NEIGHBORS value\n",
    "    for name, cent in zip(neighbors,neighborc):\n",
    "        if zone.TAZ != name:\n",
    "            otaz, dtaz = int(zone.TAZ), int(name)\n",
    "            distance = (zone.geometry.centroid.distance(cent)) / 100\n",
    "            if otaz in highway_route and dtaz in highway_route:\n",
    "                cartime = distance * speed_highway\n",
    "            else:\n",
    "                cartime = distance * speed_road\n",
    "            if otaz in [25,36,3]:\n",
    "                cartime += 3 # congestion\n",
    "            if dtaz in [25,36,3]:\n",
    "                cartime += 5 # congestion\n",
    "            if otaz in transit_line and dtaz in transit_line:\n",
    "                transit_ivtt = distance * speed_train\n",
    "                transit_ovtt = 999999\n",
    "                transit_time = transit_ivtt\n",
    "            else:\n",
    "                transit_ivtt = 999999\n",
    "                transit_ovtt = distance * speed_walk\n",
    "                transit_time = transit_ovtt\n",
    "            g.add_edge(\n",
    "                otaz, dtaz, \n",
    "                distance=distance, cartime=cartime,\n",
    "                transit_ovtt=transit_ovtt,\n",
    "                transit_ivtt=transit_ivtt,\n",
    "                transit_time=transit_time,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_shape.index = taz_shape.TAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = taz_shape.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Highway Map\n",
    "ax = taz_shape.plot(edgecolor='w')\n",
    "nx.draw_networkx_edges(    \n",
    "    g, \n",
    "    pos={i:p.coords[0] for i,p in centroids.iteritems()}, \n",
    "    ax=ax,\n",
    "    arrows=False,\n",
    "    edgelist = list(pairwise(highway_route))\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transit Map\n",
    "ax = taz_shape.plot(edgecolor='w')\n",
    "nx.draw_networkx_edges(    \n",
    "    g, \n",
    "    pos={i:p.coords[0] for i,p in centroids.iteritems()}, \n",
    "    ax=ax,\n",
    "    arrows=False,\n",
    "    edgelist = list(pairwise(transit_line))\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skim walk times\n",
    "\n",
    "WALKDIST = np.zeros([40,40])\n",
    "\n",
    "for otaz in range(1,41):\n",
    "    shortpaths = nx.shortest_path_length(g, source=otaz, weight='distance')\n",
    "    for dtaz,t in shortpaths.items():\n",
    "        WALKDIST[otaz-1,dtaz-1] = t\n",
    "    # Intrazonal\n",
    "    WALKDIST[otaz-1,otaz-1] = np.sqrt(taz_shape.loc[otaz,'geometry'].area)/100\n",
    "    \n",
    "print(WALKDIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skim car times\n",
    "\n",
    "CARTIME = np.zeros([40,40])\n",
    "CARDIST = np.zeros([40,40])\n",
    "\n",
    "for otaz in range(1,41):\n",
    "    shortpaths = nx.shortest_path(g, source=otaz, weight='cartime')\n",
    "    for dtaz,pth in shortpaths.items():\n",
    "        cartime, cardist = 0,0\n",
    "        for i,j in pairwise(pth):\n",
    "            cartime += g.edges[i,j]['cartime']\n",
    "            cardist += g.edges[i,j]['distance']\n",
    "        CARTIME[otaz-1,dtaz-1] = cartime\n",
    "        CARDIST[otaz-1,dtaz-1] = cardist\n",
    "    # Intrazonal\n",
    "    intrazonal_dist = np.sqrt(taz_shape.loc[otaz,'geometry'].area)/100\n",
    "    CARTIME[otaz-1,otaz-1] = intrazonal_dist * speed_road\n",
    "    CARDIST[otaz-1,otaz-1] = intrazonal_dist\n",
    "    if otaz in [25,36,3]:\n",
    "        CARTIME[otaz-1,otaz-1] += 5 # congestion\n",
    "        \n",
    "print(CARTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skim Transit times \n",
    "\n",
    "from itertools import tee\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "TRANSIT_IVTT = np.zeros([40,40])\n",
    "TRANSIT_OVTT = np.zeros([40,40])\n",
    "\n",
    "\n",
    "for otaz in range(1,41):\n",
    "    shortpaths = nx.shortest_path(g, source=otaz, weight='transit_time')\n",
    "    for dtaz,pth in shortpaths.items():\n",
    "        ivtt, ovtt = 0,0\n",
    "        for i,j in pairwise(pth):\n",
    "            if g.edges[i,j]['transit_ivtt'] < 999999:\n",
    "                ivtt += g.edges[i,j]['transit_ivtt']\n",
    "            else:\n",
    "                ovtt += g.edges[i,j]['transit_ovtt']\n",
    "        if ovtt == 0:\n",
    "            ovtt = np.sqrt(taz_shape.loc[otaz,'geometry'].area)/150 + np.sqrt(taz_shape.loc[dtaz,'geometry'].area)/150\n",
    "        TRANSIT_IVTT[otaz-1,dtaz-1] = ivtt\n",
    "        TRANSIT_OVTT[otaz-1,dtaz-1] = ovtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRANSIT_IVTT[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRANSIT_IVTT[5:10,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRANSIT_OVTT[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.shortest_path(g, source=30, weight='transit_time')[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSIT_OVTT[30-1,17-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRANSIT_OVTT[5:10,5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(larch.exampville.files.skims):\n",
    "    os.remove(larch.exampville.files.skims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble Skims into an OMX File\n",
    "skims_omx = larch.OMX(larch.exampville._files(seed).skims, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_omx.add_matrix('TRANSIT_IVTT', TRANSIT_IVTT)\n",
    "skims_omx.add_matrix('TRANSIT_OVTT', TRANSIT_OVTT)\n",
    "skims_omx.add_matrix('TRANSIT_FARE', (TRANSIT_IVTT>0)*2.50)\n",
    "\n",
    "skims_omx.add_matrix('WALK_DIST', WALKDIST)\n",
    "skims_omx.add_matrix('WALK_TIME', WALKDIST * speed_walk)\n",
    "skims_omx.add_matrix('BIKE_TIME', WALKDIST * speed_bike)\n",
    "\n",
    "skims_omx.add_matrix('AUTO_TIME', CARTIME)\n",
    "skims_omx.add_matrix('AUTO_COST', CARDIST * 0.35)\n",
    "skims_omx.add_matrix('AUTO_DIST', CARDIST);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_ids = np.arange(nZones)+1\n",
    "skims_omx.add_lookup('TAZ_ID', taz_ids);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_ids = np.arange(nZones)+1\n",
    "\n",
    "taz_area_types = np.full(40, 'SUB')\n",
    "taz_area_types[np.in1d(taz_ids, zones_cbd)] = 'CBD'\n",
    "taz_area_types[np.in1d(taz_ids, zones_urb)] = 'URB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_omx.add_lookup('TAZ_AREA_TYPE', taz_area_types);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_omx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_omx = larch.OMX(larch.exampville._files(seed).skims, mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tour Modes\n",
    "DA = 1\n",
    "SR = 2\n",
    "Walk = 3\n",
    "Bike = 4\n",
    "Transit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tours\n",
    "\n",
    "\n",
    "n_TOUR = PERntours.sum()\n",
    "\n",
    "TOURid      = np.arange(n_TOUR, dtype=np.int64)\n",
    "TOURper     = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURperidx  = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURhh      = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURhhidx   = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURdtaz    = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURstops   = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURmode    = np.zeros(n_TOUR, dtype=np.int64)\n",
    "TOURpurpose = np.zeros(n_TOUR, dtype=np.int64)\n",
    "\n",
    "# Work tours, then other tours\n",
    "n2 = 0\n",
    "for n1 in range(n_PER):\n",
    "    TOURper[n2:(n2 + PERntours[n1])] = PERid[n1]\n",
    "    TOURperidx[n2:(n2 + PERntours[n1])] = PERidx[n1]\n",
    "    TOURhh[n2:(n2 + PERntours[n1])] = PERhhid[n1]\n",
    "    TOURhhidx[n2:(n2 + PERntours[n1])] = PERhhidx[n1]\n",
    "    TOURpurpose[n2:(n2 + PERnworktours[n1])] = 1\n",
    "    TOURpurpose[(n2 + PERnworktours[n1]):(n2 + PERntours[n1])] = 2\n",
    "    TOURstops[n2:(n2 + PERnworktours[n1])] = np.random.choice(\n",
    "        [0, 1, 2, 3], \n",
    "        size=[PERnworktours[n1], ], \n",
    "        replace=True, \n",
    "        p=[0.8, 0.1, 0.05, 0.05],\n",
    "    ).astype(np.int64)\n",
    "    TOURstops[(n2 + PERnworktours[n1]):(n2 + PERntours[n1])] = np.random.choice(\n",
    "        [0, 1, 2, 3, 4, 5], \n",
    "        size=[PERntours[n1]-PERnworktours[n1], ], \n",
    "        replace=True, \n",
    "        p=[0.4, 0.15, 0.15, 0.15, 0.1, 0.05],\n",
    "    ).astype(np.int64)\n",
    "    n2 += PERntours[n1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERnworktours[:5], PERworks[:5], PERid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Utility by mode to various destinations\n",
    "nameModes = ['DA', 'SR', 'Walk', 'Bike', 'Transit']\n",
    "mDA = 0\n",
    "mSR = 1\n",
    "mWA = 2\n",
    "mBI = 3\n",
    "mTR = 4\n",
    "nModes = len(nameModes)\n",
    "\n",
    "nModeNests = 3\n",
    "\n",
    "paramCOST = -0.312\n",
    "paramTIME = -0.123\n",
    "paramNMTIME = -0.246\n",
    "paramDIST = -0.00357\n",
    "paramLNDIST = -0.00642\n",
    "\n",
    "paramMUcar = 0.5\n",
    "paramMUnon = 0.75\n",
    "paramMUmot = 0.8\n",
    "paramMUtop = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_retail = taz_employment.RETAIL_EMP\n",
    "zone_nonretail =  taz_employment.NONRETAIL_EMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "Util = np.zeros([n_TOUR, nZones, nModes])\n",
    "for n in range(n_TOUR):\n",
    "    # Mode\n",
    "    otazi = HH.HOMETAZ[TOURhhidx[n]] - 1\n",
    "    Util[n, :, mDA] += (\n",
    "        + skims_omx.AUTO_TIME[otazi, :] * paramTIME \n",
    "        + skims_omx.AUTO_COST[otazi, :] * paramCOST\n",
    "    )\n",
    "    if HH.INCOME[TOURhhidx[n]] >= 75000:\n",
    "        Util[n, :, mDA] += 1.0\n",
    "        Util[n, :, mTR] -= 0.5\n",
    "    Util[n, :, mSR] += (\n",
    "        + skims_omx.AUTO_TIME[otazi, :] * paramTIME \n",
    "        - 1.0 \n",
    "        + skims_omx.AUTO_COST[otazi, :] * paramCOST * 0.5\n",
    "    )\n",
    "    Util[n, :, mWA] += 3.0 + skims_omx.WALK_TIME[otazi, :] * paramNMTIME\n",
    "    Util[n, :, mBI] += -2.25 + skims_omx.BIKE_TIME[otazi, :] * paramNMTIME\n",
    "    Util[n, :, mTR] += (\n",
    "        + 1.5 \n",
    "        + skims_omx.TRANSIT_IVTT[otazi, :] * paramTIME \n",
    "        + skims_omx.TRANSIT_OVTT[otazi, :] * paramTIME * 2.2\n",
    "        + skims_omx.TRANSIT_FARE[otazi, :] * paramCOST\n",
    "    )\n",
    "    Util[n, :, mDA] += 0.33 * TOURstops[n] - 0.1\n",
    "    \n",
    "    # Destination\n",
    "    Util[n, :, :] += skims_omx.AUTO_DIST[:][otazi, :, None] * paramDIST + np.log1p(skims_omx.AUTO_DIST[:][otazi, :, None]) * paramLNDIST\n",
    "    if HH.INCOME[TOURhhidx[n]] <= 50000:\n",
    "        Util[n, :, :] += 0.75 * np.log(zone_retail * 2.71828 + zone_nonretail)[:, None]\n",
    "    else:\n",
    "        Util[n, :, :] += 0.75 * np.log(zone_retail + zone_nonretail * 2.71828)[:, None]\n",
    "    # flog('Util[n,:,:]  ...')\n",
    "    #\t\tflog('{}',Util[n,:,:])\n",
    "    # Unavails\n",
    "    if PERage[TOURperidx[n]] < 16:\n",
    "        Util[n, :, mDA] = -np.inf\n",
    "    Util[n, skims_omx.TRANSIT_FARE[otazi, :] <= 0, mTR] = -np.inf\n",
    "    Util[n, skims_omx.WALK_TIME[otazi, :] >= 60, mWA] = -np.inf\n",
    "    Util[n, skims_omx.BIKE_TIME[otazi, :] >= 60, mBI] = -np.inf\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "CPr_car = np.zeros([n_TOUR, nZones, 2])  # [DA,SR]\n",
    "CPr_non = np.zeros([n_TOUR, nZones, 2])  # [WA,BI]\n",
    "CPr_mot = np.zeros([n_TOUR, nZones, 2])  # [TR,Car]\n",
    "CPr_top = np.zeros([n_TOUR, nZones, 2])  # [Non,Mot]\n",
    "\n",
    "NLS_car = np.zeros([n_TOUR, nZones, ])\n",
    "NLS_non = np.zeros([n_TOUR, nZones, ])\n",
    "NLS_mot = np.zeros([n_TOUR, nZones, ])\n",
    "MLS_top = np.zeros([n_TOUR, nZones, ])  # Mode choice logsum\n",
    "DLS_top = np.zeros([n_TOUR, ])  # Dest choice logsum\n",
    "\n",
    "Pr_modes = np.zeros([n_TOUR, nZones, nModes])\n",
    "Pr_dest = np.zeros([n_TOUR, nZones])\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    for n in range(n_TOUR):\n",
    "        NLS_car[n, :] = paramMUcar * log(np.exp(Util[n, :, mDA] / paramMUcar) + exp(Util[n, :, mSR] / paramMUcar))\n",
    "        NLS_non[n, :] = paramMUnon * log(np.exp(Util[n, :, mWA] / paramMUnon) + exp(Util[n, :, mBI] / paramMUnon))\n",
    "        NLS_mot[n, :] = paramMUmot * log(np.exp(NLS_car[n, :] / paramMUmot) + exp(Util[n, :, mTR] / paramMUmot))\n",
    "        MLS_top[n, :] = log(exp(NLS_non[n, :]) + exp(NLS_mot[n, :]))\n",
    "        DLS_top[n] = log(np.sum(exp(MLS_top[n, :])))\n",
    "\n",
    "        Pr_dest[n, :] = exp(MLS_top[n, :] - DLS_top[n])\n",
    "\n",
    "        CPr_top[n, :, 0] = exp((NLS_non[n, :] - MLS_top[n, :]) / paramMUtop)\n",
    "        CPr_top[n, :, 1] = exp((NLS_mot[n, :] - MLS_top[n, :]) / paramMUtop)\n",
    "        CPr_mot[n, :, 0] = exp((Util[n, :, mTR] - NLS_mot[n, :]) / paramMUmot)\n",
    "        CPr_mot[n, :, 1] = exp((NLS_car[n, :] - NLS_mot[n, :]) / paramMUmot)\n",
    "        CPr_non[n, :, 0] = exp((Util[n, :, mWA] - NLS_non[n, :]) / paramMUnon)\n",
    "        CPr_non[n, :, 1] = exp((Util[n, :, mBI] - NLS_non[n, :]) / paramMUnon)\n",
    "        CPr_car[n, :, 0] = exp((Util[n, :, mDA] - NLS_car[n, :]) / paramMUcar)\n",
    "        CPr_car[n, :, 1] = exp((Util[n, :, mSR] - NLS_car[n, :]) / paramMUcar)\n",
    "\n",
    "        Pr_modes[n, :, mTR] = CPr_mot[n, :, 0] * CPr_top[n, :, 1] * Pr_dest[n, :]\n",
    "        Pr_modes[n, :, mWA] = CPr_non[n, :, 0] * CPr_top[n, :, 0] * Pr_dest[n, :]\n",
    "        Pr_modes[n, :, mBI] = CPr_non[n, :, 1] * CPr_top[n, :, 0] * Pr_dest[n, :]\n",
    "        Pr_modes[n, :, mDA] = CPr_car[n, :, 0] * CPr_mot[n, :, 1] * CPr_top[n, :, 1] * Pr_dest[n, :]\n",
    "        Pr_modes[n, :, mSR] = CPr_car[n, :, 1] * CPr_mot[n, :, 1] * CPr_top[n, :, 1] * Pr_dest[n, :]\n",
    "\n",
    "Pr_modes[np.isnan(Pr_modes)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "## Choices\n",
    "for n in range(n_TOUR):\n",
    "    try:\n",
    "        ch = np.random.choice(nModes * nZones, replace=True, p=Pr_modes[n, :, :].ravel())\n",
    "    except:\n",
    "        print(\"total prob = {}\", Pr_modes[n, :, :].sum())\n",
    "        raise\n",
    "    dtazi = ch // nModes\n",
    "    modei = ch - (dtazi * nModes)\n",
    "    TOURdtaz[n] = dtazi + 1\n",
    "    TOURmode[n] = modei + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "f_tour = pd.DataFrame.from_dict(\n",
    "    dict([\n",
    "        ('TOURID', TOURid),\n",
    "        ('HHID', TOURhh),\n",
    "        ('PERSONID', TOURper),\n",
    "        ('DTAZ', TOURdtaz),\n",
    "        ('TOURMODE', TOURmode),\n",
    "        ('TOURPURP', TOURpurpose),\n",
    "        ('N_STOPS', TOURstops),\n",
    "    ])\n",
    ")\n",
    "# f_tour_filename = os.path.join(directory, 'exampville_tours.csv')\n",
    "# f_tour.to_csv(f_tour_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "f_tour.set_index('TOURID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "DA = 1\n",
    "SR = 2\n",
    "Walk = 3\n",
    "Bike = 4\n",
    "Transit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "dfs = larch.DataFrames(\n",
    "    co=f_tour, \n",
    "    alt_codes=[DA,SR,Walk,Bike,Transit], \n",
    "    alt_names=['DA','SR','Walk','Bike','Transit'],\n",
    "    ch_name='TOURMODE',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "dfs.data_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "dfs.choice_avail_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "source": [
    "## Roll Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Trips Per Person\n",
    "f_tour['N_TRIPS'] = 2+f_tour['N_STOPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "f_tour['N_TRIPS_HBW'] = (f_tour.TOURPURP==1)*2 - ((f_tour.TOURPURP==1)&(f_tour.N_STOPS>0))\n",
    "f_tour['N_TRIPS_HBO'] = ((f_tour.TOURPURP==1)*2 - ((f_tour.TOURPURP==1)&(f_tour.N_STOPS>0))==1) + (f_tour.TOURPURP==2)*2\n",
    "f_tour['N_TRIPS_NHB'] = f_tour['N_TRIPS'] - f_tour['N_TRIPS_HBW'] - f_tour['N_TRIPS_HBO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "f_tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "PER.index = PER.PERSONID\n",
    "\n",
    "for ttype in [\n",
    "        'N_TRIPS', \n",
    "        'N_TRIPS_HBW',\n",
    "        'N_TRIPS_HBO',\n",
    "        'N_TRIPS_NHB',\n",
    "]:\n",
    "    PER[ttype] = f_tour.groupby('PERSONID')[ttype].sum()\n",
    "    PER[ttype] = PER[ttype].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "HH.index = HH.HHID\n",
    "\n",
    "for ttype in [\n",
    "        'N_TRIPS', \n",
    "        'N_TRIPS_HBW',\n",
    "        'N_TRIPS_HBO',\n",
    "        'N_TRIPS_NHB',\n",
    "]:\n",
    "    HH[ttype] = PER.groupby('HHID')[ttype].sum()\n",
    "    HH[ttype] = HH[ttype].fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "HH['N_WORKERS'] = PER.groupby('HHID').WORKS.sum()\n",
    "HH['N_WORKERS'] = HH.N_WORKERS.fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "HH['N_WORKERS'].statistics(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "PER.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "w, b1, b2 = np.histogram2d(\n",
    "    PER.N_VEHICLES, \n",
    "    PER.N_WORK_TOURS, \n",
    "    bins=[\n",
    "        np.array([0,1,2,3,4,5,6,7])-0.5, \n",
    "        [0,1,2,3,4,5,6,7,8,9,10,12,14,16],\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "ww = pd.DataFrame(\n",
    "        w,\n",
    "        index=[0,1,2,3,4,5,6],\n",
    "        columns=[f\"{i}\" for i,j in zip(b2[:-1], b2[1:])],\n",
    "    )\n",
    "\n",
    "ww.T.div(ww.T.sum(0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(\n",
    "    data=pd.DataFrame(\n",
    "        w,\n",
    "        index=[0,1,2,3,4,5,6],\n",
    "        columns=[f\"{i}\" for i,j in zip(b2[:-1], b2[1:])],\n",
    "    ),\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "hh_means = HH.groupby('HOMETAZ')[['INCOME', 'N_VEHICLES', 'HHSIZE',\n",
    "       'N_TRIPS', 'N_TRIPS_HBW', 'N_TRIPS_HBO', 'N_TRIPS_NHB',\n",
    "       'N_WORKERS']].mean().add_prefix('MEAN_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "hh_sums = HH.groupby('HOMETAZ')[['N_VEHICLES', 'HHSIZE',\n",
    "       'N_TRIPS', 'N_TRIPS_HBW', 'N_TRIPS_HBO', 'N_TRIPS_NHB',\n",
    "       'N_WORKERS']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "hh_sums.rename(columns={\n",
    "    'HHSIZE':\"TOTAL_POP\", \n",
    "    'N_WORKERS':\"TOTAL_WORKERS\", \n",
    "    'N_VEHICLES':\"TOTAL_VEHICLES\", \n",
    "    'N_TRIPS':\"TOTAL_TRIPS\", \n",
    "    'N_TRIPS_HBW':'TOTAL_TRIPS_HBW', \n",
    "    'N_TRIPS_HBO':\"TOTAL_TRIPS_HBO\", \n",
    "    'N_TRIPS_NHB':\"TOTAL_TRIPS_NHB\",\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "taz_agg = pd.concat([hh_means, hh_sums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "taz_agg['N_HOUSEHOLDS'] = HH.groupby('HOMETAZ').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "taz_agg.index.name = \"TAZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Drop income and n_veh from persons, they are hh attributes for our demos\n",
    "\n",
    "PER.drop(['N_VEHICLES','INCOME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "stop\n",
    "HH.to_csv(larch.exampville._files(seed).hh, index=False)\n",
    "PER.to_csv(larch.exampville._files(seed).person, index=False)\n",
    "f_tour.to_csv(larch.exampville._files(seed).tour)\n",
    "taz_agg.to_csv(larch.exampville._files(seed).demographics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "larch.exampville._files(seed).demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
